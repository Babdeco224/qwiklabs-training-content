name: 'Big Data & ML Fundamentals Lab 4: Recommendations ML with Dataproc v1.3 - Test'
description: "The script downloads the code from the git.\nCreates a Cloud SQL instance and launches Dataproc.\nTrains and apply ML model written in PySpark to create product recommendations.\nCopies the file to Cloud Storage bucket and submits the job to run the ML model."
deployment_manager_time: 600
run_regularly: true
failure_alerts_only: true
subscribers:
 - training-qa-testers@google.com
 - ajaych@google.com
steps:
 - command: |
     #Task1: Download the code from github
     sudo apt-get update
     sudo apt-get -y -qq install git
     git clone https://github.com/GoogleCloudPlatform/training-data-analyst
     cd training-data-analyst/CPB100/lab3a
     #Create the Cloud Storage bucket
     gsutil mb -l us-central1 gs://$PROJECT/
     #Stage .sql and .csv files into Cloud Storage
     gsutil cp cloudsql/* gs://$PROJECT/sql/
 - command: |
     #Task2: Create Cloud SQL instance
     gcloud sql instances create rentals --authorized-networks=$(bash ./find_my_ip.sh) --zone=us-central1-a
     # Set the password to the Cloud SQL instance
     gcloud sql users set-password root --host=% --instance=rentals --password=root
     cd
     # Give permission to the Bucket and its data
     INSTANCE_SA=$(gcloud sql instances describe rentals --format='value(serviceAccountEmailAddress)')
     gsutil acl ch -u $INSTANCE_SA:W gs://$PROJECT
     gsutil acl ch -u $INSTANCE_SA:R gs://$PROJECT/sql/*
     # Import tables in SQL instance
     gcloud sql import sql rentals gs://$PROJECT/sql/table_creation.sql --quiet
     # Populate tables
     gcloud sql import csv rentals gs://$PROJECT/sql/accommodation.csv --database=recommendation_spark --table=Accommodation --quiet
     gcloud sql import csv rentals gs://$PROJECT/sql/rating.csv --database=recommendation_spark --table=Rating --quiet
     # Enable Dataproc API
     gcloud -q services enable dataproc.googleapis.com
 - command: |
     #Task3: Create Dataproc Cluster
     gcloud dataproc clusters create test-cluster --region='global' --zone='us-central1-a' --master-machine-type='n1-standard-2' --worker-machine-type='n1-standard-2'
     # Check the status of master and worker instances
     while :
     do
     INSTANCE_STATUS=$(gcloud compute instances list --filter 'status=RUNNING AND name:test-cluster-*' --format 'value(Name)')
     if [[ $INSTANCE_STATUS = *"m"*"w-0"*"w-1" ]]
     then
      break
     fi
     sleep 10s
     done
     cd ~/training-data-analyst/CPB100/lab3b
     # Authorize all the Dataproc nodes to be able to access your Cloud SQL instance
     yes Y | bash authorize_dataproc.sh test-cluster us-central1-a 2
     # Replace the values in file
     INSTANCE_IP=$(gcloud sql instances describe rentals --format='value(ipAddresses[ipAddress])')
     sed -i "s/104.155.188.32/$INSTANCE_IP/g" sparkml/train_and_apply.py
 - command: |
    #Task4: Copy the file to Cloud Storage bucket and submit the job to run the ML model
    while :
    do
    gsutil cp sparkml/tr*.py gs://$PROJECT/
    JOB_ID=$(gcloud dataproc jobs submit pyspark gs://$PROJECT/train_and_apply.py --cluster=test-cluster --region=global --format 'value  (status.state)')
    if [ $JOB_ID == 'DONE' ]
    then
     break
    fi
    done
