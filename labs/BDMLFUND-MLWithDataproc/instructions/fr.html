<h1>Machine learning pour les recommandations avec Dataproc</h1>
<h2>Aperçu</h2>
<p>Dans cet atelier, vous allez exécuter un modèle de recommandations basées sur le machine learning à l'aide de Dataproc.</p>
<h3>Objectifs de l'atelier</h3>
<p>Au cours de cet atelier, vous allez apprendre à réaliser les opérations suivantes :</p>
<ul>
<li>
<p>Lancer Dataproc</p>
</li>
<li>
<p>Exécuter des tâches Spark ML à l'aide de Dataproc</p>
</li>
</ul>
<h2>Présentation</h2>
<p>Au cours de cet atelier, vous allez entraîner un modèle de recommandations basées sur le machine learning (précédentes notes attribuées par les utilisateurs) à l'aide de Dataproc. Vous appliquerez ensuite ce modèle pour créer une liste de recommandations pour chaque utilisateur figurant dans la base de données.</p>
<p>Au cours de cet atelier, vous allez apprendre à réaliser les opérations suivantes :</p>
<ul>
<li>Lancer Dataproc</li>
<li>Entraîner et appliquer un modèle de ML écrit dans PySpark pour créer des recommandations de produits</li>
<li>Explorer les lignes insérées dans Cloud SQL</li>
</ul>
<fragment path="/fragments/startqwiklab"></fragment>
<h2>Configuration</h2>
<fragment path="/fragments/cloudshell"></fragment>
<h2>Tâche 1 : Créer des éléments</h2>
<p>Nous allons commencer par cloner le dépôt du code en créant un bucket de stockage dans le projet GCP et en organisant certains fichiers. Ces étapes sont similaires aux étapes que vous avez effectuées lors de l'atelier précédent.</p>
<ol>
<li>
<p>Dans Cloud Shell, clonez le dépôt à l'aide de la commande suivante :</p>
<pre><code class="language-bash">git clone https://github.com/GoogleCloudPlatform/training-data-analyst
</code></pre>
<p>Le code est alors téléchargé depuis GitHub.</p>
</li>
<li>
<p>Accédez au dossier correspondant à cet atelier :</p>
<pre><code class="language-bash">cd training-data-analyst/CPB100/lab3a
</code></pre>
</li>
<li>
<p>Dans la console GCP, accédez au <strong>menu de navigation</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), puis cliquez sur <strong>Storage</strong> (Stockage).</p>
</li>
<li>
<p>Cliquez sur <strong>Create bucket</strong> (Créer un compartiment).</p>
</li>
<li>
<p>Dans le champ <strong>Name</strong> (Nom), saisissez votre <strong>ID du projet</strong>, puis cliquez sur <strong>Create</strong> (Créer). Sélectionnez <strong>Set object-level and bucket-level permissions</strong> (Définir des autorisations au niveau de l'objet et du bucket) en dessous de <strong>Access control model</strong> (Modèle de contrôle des accès), puis cliquez sur <strong>Create</strong> (Créer). Pour trouver votre <strong>ID du projet</strong>, cliquez sur le projet dans le menu supérieur de la console GCP et copiez la valeur sous <strong>ID</strong> pour le projet sélectionné.</p>
</li>
<li>
<p>Pour finir, placez les fichiers de données et de définition de table dans Cloud Storage afin de pouvoir les importer ultérieurement dans Cloud SQL. Depuis Cloud Shell, dans le répertoire lab3a, saisissez la commande suivante et remplacez <code>&lt;NOM-BUCKET&gt;</code> par le nom du bucket que vous venez de créer :</p>
<pre><code class="language-bash">gsutil cp cloudsql/* gs://&lt;NOM-BUCKET&gt;/sql/
</code></pre>
</li>
<li>
<p>Dans la section "Storage" (Stockage) de la console GCP, accédez à votre bucket, puis vérifiez que les fichiers .sql et .csv apparaissent dans Cloud Storage.</p>
</li>
</ol>
<h2>Tâche 2 : Créer une instance Cloud SQL</h2>
<p>Pour créer une instance Cloud SQL, procédez comme suit :</p>
<ol>
<li>
<p>Dans la console GCP, accédez au <strong>menu de navigation</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), puis cliquez sur <strong>SQL</strong> dans la section "Storage" (Stockage).</p>
</li>
<li>
<p>Cliquez ensuite sur <strong>Create Instance</strong> (Créer une instance).</p>
</li>
<li>
<p><strong>Choose MySQL</strong>(Sélectionner MySQL). Cliquez sur <strong>Next</strong>(Suivant) si nécessaire. Cliquez sur <strong>Configure MySQL Development</strong>(Configurer Développement de MySQL) ou <strong>Choose Second Generation</strong>(Elegir la segunda generación).</p>
</li>
<li>
<p>Dans le champ <strong>Instance ID</strong> (ID d'instance), saisissez <strong>rentals</strong>.</p>
<p><img src="img/ab1cdf08212ecadf.png" alt="ab1cdf08212ecadf.png"></p>
</li>
<li>
<p>Faites défiler la page vers le bas, puis spécifiez un mot de passe racine.  Pour ne pas risquer de l'oublier, notez ce mot de passe (ne le faites pas en dehors de cet atelier).</p>
</li>
<li>
<p>Faites défiler la page, puis cliquez sur <strong>Set Connectivity</strong>(Définir la connectivité) ou <strong>Show configuration options</strong>(Mostrar opciones de configuración), premier (si nécessaire) puis Cliquez sur <strong>Authorize networks</strong>(Réseaux autorisés) > <strong>+Add network</strong>(Ajouter réseau).</p>
<p><img src="img/ad40ddbc1cfc0a81.png" alt="ad40ddbc1cfc0a81.png"></p>
</li>
<li>
<p>Dans Cloud Shell, vérifiez que vous êtes dans le répertoire <strong>lab3a</strong> et trouvez votre adresse IP en saisissant la commande suivante :</p>
<pre><code class="language-bash">bash ./find_my_ip.sh
</code></pre>
</li>
<li>
<p>Dans la boîte de dialogue <strong>New network</strong> (Nouveau réseau), saisissez un <strong>nom</strong> (facultatif) ainsi que l'<strong>adresse IP</strong>(Réseau) obtenue lors de l'étape précédente. Cliquez sur <strong>Done</strong> (OK).</p>
<p><img src="img/54d82efbc516bceb.png" alt="54d82efbc516bceb.png"></p>
<p><strong>Remarque </strong>: Si vous perdez l'accès à votre VM Cloud Shell pour cause d'inactivité, vous devrez à nouveau autoriser l'accès à votre nouvelle VM Cloud Shell avec Cloud SQL. Pour plus de facilité, le répertoire lab3a comprend un script exécutable nommé <strong>authorize_cloudshell.sh</strong>.</p>
</li>
<li>
<p>Cliquez sur <strong>Create</strong> (Créer) pour créer l'instance. Le provisionnement de votre instance Cloud SQL devrait prendre environ une minute.</p>
</li>
<li>
<p>Notez l'<strong>adresse IP principale</strong>('adresse IP publique) de votre instance Cloud SQL (depuis la fenêtre du navigateur).</p>
</li>
</ol>
<h2>Tâche 3 : Créer et remplir des tables</h2>
<p>Pour importer des définitions de tables depuis Cloud Storage, procédez comme suit :</p>
<ol>
<li>
<p>Cliquez sur <strong>rentals</strong> pour afficher les détails de votre instance Cloud SQL.</p>
</li>
<li>
<p>Cliquez sur <strong>Import</strong> (Importer).</p>
</li>
<li>
<p>Cliquez sur <strong>Browse</strong> (Parcourir). Une liste de buckets s'affiche alors. Cliquez sur le bucket que vous avez créé, puis accédez à <strong>sql</strong>. Cliquez ensuite sur <strong>table_creation.sql</strong>, puis sur <strong>Select</strong> (Sélectionner).</p>
</li>
<li>
<p>Cliquez sur <strong>Import</strong> (Importer).</p>
</li>
<aside class="warning"><p>
<strong>Note</strong>: Si l'import de <code>.sql</code> ou <code>.csv</code> échoue au premier essai, relancez l'import.
</p>
</aside>
<li>
<p>Pour importer ensuite des fichiers CSV à partir de Cloud Storage, cliquez sur <strong>Import</strong> (Importer).</p>
</li>
<li>
<p>Cliquez sur <strong>Browse</strong> (Parcourir) et accédez à <strong>sql</strong>. Cliquez sur <strong>accommodation.csv</strong>, puis sur <strong>Select</strong> (Sélectionner).</p>
</li>
<li>
<p>Remplissez le reste de la boîte de dialogue comme suit :</p>
<p>Dans le champ <strong>Database</strong> (Base de données), sélectionnez <strong>recommendation_spark</strong>.</p>
<p>Dans le champ <strong>Table</strong>, saisissez <strong>Accommodation</strong>.</p>
<p><img src="img/c899adb7fdb39f17.png" alt="c899adb7fdb39f17.png"></p>
</li>
<li>
<p>Cliquez sur <strong>Import</strong> (Importer).</p>
</li>
<li>
<p>Répétez le processus d'importation (étapes 5 à 8) pour le fichier <strong>rating.csv</strong>, mais saisissez <strong>Rating</strong> comme nom dans le champ <strong>Table</strong>.</p>
</li>
</ol>
<h2>Tâche 4 : Lancer Dataproc</h2>
<p>Pour lancer Dataproc et le configurer pour que chaque machine du cluster puisse accéder à Cloud SQL, procédez comme suit :</p>
<ol>
<li>
<p>Dans la console GCP, accédez au <strong>menu de navigation</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), puis cliquez sur <strong>SQL</strong> et notez la région de votre instance Cloud SQL :</p>
<p><img src="img/fc8f254ae64a75b4.png" alt="fc8f254ae64a75b4.png"></p>
<p>Dans l'instantané ci-dessus, la région est <code>us-central1</code>.</p>
</li>
<li>
<p>Dans la console GCP, accédez au <strong>menu de navigation</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), cliquez sur <strong>Dataproc</strong>, puis sur <strong>Enable API</strong> (Activer l'API) si vous y êtes invité. Une fois l'API activée, cliquez sur <strong>Create cluster</strong> (Créer un cluster).</p>
</li>
<li>
<p>Laisser la région telle quelle, c'est-à-dire globale, changez la zone pour qu'elle se trouve dans la même région que votre instance Cloud SQL. Cela permet de minimiser la latence du réseau entre le cluster et la base de données.</p>
</li>
<li>
<p>Pour le <strong>nœud maître</strong>, dans le champ <strong>Machine type</strong> (Type de machine), cliquez sur <strong>2 vCPU (n1-standard-2)</strong> (2 processeurs virtuels (n1-standard-2)).</p>
</li>
<li>
<p>Pour les <strong>nœuds de calcul</strong>, dans le champ <strong>Machine type</strong> (Type de machine), cliquez sur <strong>2 vCPU (n1-standard-2)</strong> (2 processeurs virtuels (n1-standard-2)).</p>
</li>
<li>
<p>Conservez les autres valeurs par défaut, puis cliquez sur <strong>Create</strong> (Créer).  Le provisionnement de votre cluster prendra une à deux minutes.</p>
</li>
<li>
<p>Notez le <strong>nom</strong>, la <strong>zone</strong> et le <strong>nombre total de nœuds de calcul</strong> dans votre cluster.</p>
</li>
<li>
<p>Dans Cloud Shell, accédez au dossier correspondant à cet atelier et autorisez tous les nœuds Dataproc à accéder à votre instance Cloud SQL, en remplaçant <code>&lt;Nom-cluster&gt;</code>, <code>&lt;Zone&gt;</code> et <code>&lt;Nombre-total-de-nœuds-de-calcul&gt;</code> par les valeurs que vous avez notées lors de l'étape précédente :</p>
<pre><code class="language-bash">cd ~/training-data-analyst/CPB100/lab3b
bash authorize_dataproc.sh &lt;Nom-cluster&gt; &lt;Zone&gt; &lt;Nombre-total-de-nœuds-de-calcul&gt;
</code></pre>
<p>Lorsque vous y êtes invité, saisissez <strong>Y</strong>, puis appuyez sur <strong>Enter</strong> pour continuer.</p>
</li>
</ol>
<h2>Tâche 5 : Exécuter le modèle de ML</h2>
<p>Pour créer un modèle entraîné et l'appliquer à tous les utilisateurs du système, procédez comme suit :</p>
<ol>
<li>
<p>Modifiez le fichier d'entraînement du modèle à l'aide de <code>nano</code> :</p>
<pre><code class="language-bash">nano sparkml/train_and_apply.py
</code></pre>
</li>
<li>
<p>Modifiez les champs portant la mention "CHANGE" (Modifier) en haut du fichier (utilisez la flèche vers le bas pour faire défiler le fichier) pour qu'ils correspondent à votre configuration Cloud SQL (reportez-vous aux sections précédentes de cet atelier où vous avez noté les informations requises). Ensuite, enregistrez le fichier en appuyant sur <strong>Ctrl+O</strong> et <strong>Enter</strong>, puis quittez la page en utilisant <strong>Ctrl+X</strong>.</p>
</li>
<li>
<p>Copiez ce fichier dans votre bucket Cloud Storage à l'aide de la commande suivante :</p>
<pre><code class="language-bash">gsutil cp sparkml/tr*.py gs://&lt;nom-bucket&gt;/
</code></pre>
</li>
<li>
<p>Dans la console <strong>Dataproc</strong>, cliquez sur <strong>Jobs</strong> (Tâches).</p>
<p><img src="img/8508ce301ff584c3.png" alt="8508ce301ff584c3.png"></p>
</li>
<li>
<p>Cliquez sur <strong>Submit job</strong> (Envoyer une tâche).</p>
</li>
<li>
<p>Dans le champ <strong>Job type</strong> (Type de tâche), sélectionnez <strong>PySpark</strong>, puis précisez l'emplacement du fichier Python que vous avez importé dans votre bucket dans le champ <strong>Main python file</strong> (Classe principale ou fichier JAR).</p>
<p><img src="img/e15bafe2c29956b5.png" alt="e15bafe2c29956b5.png"></p>
<p><code>gs://&lt;nom-bucket&gt;/train_and_apply.py</code></p>
</li>
<li>
<p>Cliquez sur <strong>Submit</strong> (Envoyer) et attendez que l'état de la tâche passe de <code>Running</code> (En cours d'exécution) à <code>Succeeded</code> (Réussie). Cela prendra jusqu'à cinq minutes.</p>
<p><img src="img/af55e2a91617b1d5.png" alt="af55e2a91617b1d5.png"></p>
<p>Si la tâche a <code>échoué</code>, veuillez résoudre les problèmes en utilisant les journaux et corriger les erreurs. Vous devrez peut-être importer de nouveau le fichier Python modifié dans Cloud Storage et cloner la tâche échouée pour la renvoyer.</p>
</li>
</ol>
<h2>Tâche 6 : Explorer les lignes insérées</h2>
<ol>
<li>
<p>Dans la console GCP, accédez au <strong>menu de navigation</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), puis cliquez sur <strong>SQL</strong> dans la section "Storage" (Stockage).</p>
</li>
<li>
<p>Cliquez sur <strong>rentals</strong> pour afficher les détails de votre instance Cloud SQL.</p>
</li>
<li>
<p>Sous <strong>Connect to this instance</strong> (Se connecter à cette instance), cliquez sur <strong>Connect using Cloud Shell</strong> (Se connecter via Cloud Shell). Cela ouvrira un nouvel onglet Cloud Shell. Dans l'onglet Cloud Shell, appuyez sur <strong>Entrée</strong>.</p>
<p>Un délai de quelques minutes peut être nécessaire pour ajouter votre adresse IP à la liste blanche des connexions entrantes.</p>
</li>
<li>
<p>Lorsque vous y êtes invité, saisissez le mot de passe racine que vous avez configuré, puis appuyez sur <strong>Enter</strong>.</p>
</li>
<li>
<p>À l'invite mysql, saisissez la commande suivante :</p>
<pre><code class="language-bash">use recommendation_spark;
</code></pre>
<p>La base de données sera alors définie dans la session mysql.</p>
</li>
<li>
<p>Trouvez les recommandations pour certains utilisateurs :</p>
<pre><code class="language-bash">select r.userid, r.accoid, r.prediction, a.title, a.location, a.price, a.rooms, a.rating, a.type from Recommendation as r, Accommodation as a where r.accoid = a.id and r.userid = 10;
</code></pre>
<p>Nous recommandons ces cinq logements. Notez que la qualité des recommandations n'est pas excellente, car notre ensemble de données était limité (voyez comme les notes prédites ne sont pas très élevées). Toutefois, l'objectif de cet atelier est d'illustrer le processus à suivre pour créer des recommandations de produits.</p>
</li>
</ol>
<fragment path="/fragments/endqwiklab"></fragment>
<h5>Dernière mise à jour du manuel : 25 février 2019</h5>
<h5>Dernier test de l'atelier : 25 février 2019</h5>
<fragment path="/fragments/copyright"></fragment>
