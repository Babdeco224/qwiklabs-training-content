<h1>AA para recomendaciones con Dataproc</h1>
<h2>Descripción general</h2>
<p>En este lab, utilizará el aprendizaje automático para hacer recomendaciones con Dataproc.</p>
<h3>Qué aprenderá</h3>
<p>En este lab, aprenderá a realizar las siguientes actividades:</p>
<ul>
<li>
<p>Iniciar Dataproc</p>
</li>
<li>
<p>Ejecutar trabajos de SparkML con Dataproc</p>
</li>
</ul>
<h2>Introducción</h2>
<p>En este lab, usará Dataproc para entrenar el modelo de aprendizaje automático de recomendaciones con las calificaciones anteriores de los usuarios. Luego, aplicará ese modelo a fin de crear una lista de recomendaciones para cada usuario de la base de datos.</p>
<p>En este lab, aprenderá a realizar las siguientes actividades:</p>
<ul>
<li>Iniciar Dataproc</li>
<li>Entrenar y aplicar el modelo del AA escrito en PySpark para crear recomendaciones de productos</li>
<li>Explorar las filas insertadas en Cloud SQL</li>
</ul>
<fragment path="/fragments/startqwiklab"></fragment>
<h2>Configuración</h2>
<fragment path="/fragments/cloudshell"></fragment>
<h2>Tarea 1: Cree recursos</h2>
<p>Para comenzar, debe clonar el repositorio de código, crear un depósito de almacenamiento en el proyecto de GCP y stage algunos archivos. Estos pasos son similares a los que realizó en el lab anterior.</p>
<ol>
<li>
<p>En Cloud Shell, use el siguiente comando para clonar el repositorio:</p>
<pre><code class="language-bash">git clone https://github.com/GoogleCloudPlatform/training-data-analyst
</code></pre>
<p>Eso hará que se descargue el código de github.</p>
</li>
<li>
<p>Navegue a la carpeta correspondiente a este lab:</p>
<pre><code class="language-bash">cd training-data-analyst/CPB100/lab3a
</code></pre>
</li>
<li>
<p>En GCP Console, en el <strong>menú de navegación</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), haga clic en <strong>Storage</strong>.</p>
</li>
<li>
<p>Haga clic en <strong>Crear segmento</strong>.</p>
</li>
<li>
<p>En <strong>Nombre</strong>, ingrese su <strong>ID del proyecto</strong>. Seleccione <strong>Configurar permisos en los niveles de objeto y de segmento</strong> en <strong>Modelo de control de acceso</strong>,  luego, haga clic en <strong>Crear</strong>. Para buscar su <strong>ID del proyecto</strong>, haga clic en el proyecto en el menú superior de GCP Console y copie el valor que aparece debajo del <strong>ID</strong> del proyecto seleccionado.</p>
</li>
<li>
<p>Por último, stage la definición de la tabla y los archivos de datos en Cloud Storage a fin de poder importarlos a Cloud SQL desde Cloud Shell en el directorio lab3a. Para ello, escriba lo siguiente y reemplace<code> &lt;BUCKET-NAME&gt;</code> con el nombre del depósito que acaba de crear:</p>
<pre><code class="language-bash">gsutil cp cloudsql/* gs://&lt;BUCKET-NAME&gt;/sql/
</code></pre>
</li>
<li>
<p>En GCP Console, vaya a Storage, navegue a su depósito y verifique que los archivos .sql y .csv existan ahora en Cloud Storage.</p>
</li>
</ol>
<h2>Tarea 2: Cree una instancia de Cloud SQL</h2>
<p>Para crear una instancia de Cloud SQL, siga estos pasos:</p>
<ol>
<li>
<p>En GCP Console, en el <strong>menú de navegación</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), haga clic en <strong>SQL</strong> en la sección Storage.</p>
</li>
<li>
<p>Haga clic en <strong>Crear una instancia</strong>.</p>
</li>
<li>
<p><strong>Elegir MySQL</strong>. Haga clic en <strong>Siguiente</strong> si es necesario. Haga clic en <Strong>Configurar desarrollo: MySQL</strong> o <Strong>Elegir la segunda generación</strong>.</p>
</li>
<li>
<p>En <strong>ID de instancia</strong>, escriba <strong>rentals</strong>.</p>
<p><img src="img/ab1cdf08212ecadf.png" alt="ab1cdf08212ecadf.png"></p>
</li>
<li>
<p>Desplácese hacia abajo y especifique una contraseña raíz.  Para no olvidarla, anótela. Tenga en cuenta que esto no debe hacerse en una situación real.</p>
</li>
<li>
<p>Desplácese hacia abajo y, haga clic en <strong>Configurar conexión</strong> o <strong>Mostrar opciones de configuración</strong>, primero (si es necesario) después Haga clic en <strong>Redes autorizadas</strong> > <strong>+ Añadir red</strong>.</p>
<p><img src="img/ad40ddbc1cfc0a81.png" alt="ad40ddbc1cfc0a81.png"></p>
</li>
<li>
<p>En Cloud Shell, asegúrese de estar en el directorio <strong>lab3a</strong> y escriba lo siguiente para buscar la dirección IP:</p>
<pre><code class="language-bash">bash ./find_my_ip.sh
</code></pre>
</li>
<li>
<p>En el cuadro de diálogo <strong>Nueva red</strong>, ingrese un <strong>nombre</strong> opcional y el resultado de la <strong>Red</strong> del paso anterior. Haga clic en <strong>Listo</strong>.</p>
<p><img src="img/54d82efbc516bceb.png" alt="54d82efbc516bceb.png"></p>
<p><strong>Nota</strong>: Si pierde su VM de Cloud Shell por inactividad, deberá volver a autorizar una nueva con Cloud SQL. Para su comodidad, lab3a incluye una secuencia de comandos llamada <strong>authorize_cloudshell.sh</strong> que puede ejecutar.</p>
</li>
<li>
<p>Haga clic en <strong>Crear</strong> para crear la instancia. Su instancia de Cloud SQL demorará aproximadamente un minuto en aprovisionarse.</p>
</li>
<li>
<p>Tome nota de la <strong>dirección IP pública</strong> de su instancia de Cloud SQL (en la ventana del navegador).</p>
</li>
</ol>
<h2>Tarea 3: Cree y propague tablas</h2>
<p>Para importar definiciones de tablas desde Cloud Storage, siga estos pasos:</p>
<ol>
<li>
<p>Haga clic en <strong>Rentals</strong> para ver los detalles de su instancia de Cloud SQL.</p>
</li>
<li>
<p>Haga clic en <strong>Importar</strong>.</p>
</li>
<li>
<p>Haga clic en <strong>Examinar</strong>. Aparecerá una lista de depósitos. Haga clic en el depósito que creó y navegue a <strong>sql</strong>. Luego, haga clic en <strong>table_creation.sql</strong> y en <strong>Seleccionar</strong>.</p>
</li>
<li>
<p>Haga clic en <strong>Importar</strong>.</p>
</li>
<aside class="warning"><p>
<strong>Nota</strong>: Si la importación de los archivos <code>.sql</code> o <code>.csv</code> falla en el primer intento, intente nuevamente.
</p>
</aside>
<li>
<p>A continuación, para importar archivos CSV desde Cloud Storage, haga clic en <strong>Importar</strong>.</p>
</li>
<li>
<p>Haga clic en <strong>Examinar</strong> y navegue a <strong>sql</strong>. Luego, haga clic en <strong>accommodation.csv</strong> y en <strong>Seleccionar</strong>.</p>
</li>
<li>
<p>Llene el resto del diálogo con la siguiente información:</p>
<p>En <strong>Base de Datos</strong>, seleccione <strong>recommendation_spark</strong>.</p>
<p>En <strong>Tabla</strong>, escriba <strong>Accommodation</strong>.</p>
<p><img src="img/c899adb7fdb39f17.png" alt="c899adb7fdb39f17.png"></p>
</li>
<li>
<p>Haga clic en <strong>Importar</strong>.</p>
</li>
<li>
<p>Repita el proceso de importación (pasos 5 a 8) para <strong>rating.csv</strong>, pero en <strong>Tabla</strong>, escriba <strong>Rating</strong>.</p>
</li>
</ol>
<h2>Tarea 4: Inicie Dataproc</h2>
<p>Para iniciar Dataproc y configurarlo de modo que cada una de las máquinas del clúster pueda acceder a Cloud SQL, siga estos pasos:</p>
<ol>
<li>
<p>En GCP Console, en el <strong>menú de navegación</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), haga clic en <strong>SQL</strong> y tome nota de la región de la instancia de Cloud SQL:</p>
<p><img src="img/fc8f254ae64a75b4.png" alt="fc8f254ae64a75b4.png"></p>
<p>En la instantánea anterior, la región es <code>us-central1</code>.</p>
</li>
<li>
<p>En GCP Console, en el <strong>menú de navegación</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), haga clic en <strong>Dataproc</strong> y en <strong>Enable API</strong> si se le solicita. Una vez habilitada la API, haga clic en <strong>Crear clúster</strong>.</p>
</li>
<li>
<p>Mantenga "Region" como "Global", Cambie la zona para que esté en la misma región que su instancia de Cloud SQL. Esto reducirá la latencia de la red entre el clúster y la base de datos.</p>
</li>
<li>
<p>En <strong>Nodo maestro</strong>, en <strong>Tipo de máquina</strong>, seleccione <strong>2 CPU virtuales (n1-standard-2)</strong>.</p>
</li>
<li>
<p>En <strong>Nodos de trabajo</strong>, en <strong>Tipo de máquina</strong>, seleccione <strong>2 CPU virtuales (n1-standard-2)</strong>.</p>
</li>
<li>
<p>Deje todos los otros valores con su configuración predeterminada y haga clic en <strong>Crear</strong>.  El clúster demorará entre 1 y 2 minutos en aprovisionarse.</p>
</li>
<li>
<p>Tome nota de los valores <strong>Nombre</strong>, <strong>Zona</strong> y <strong>Número total de nodos de trabajo</strong> del clúster.</p>
</li>
<li>
<p>En Cloud Shell, navegue a la carpeta correspondiente a este lab y autorice todos los nodos de Dataproc a fin de que puedan acceder a su instancia de Cloud SQL. Para ello, reemplace <code>&lt;Cluster-Name&gt;</code>, <code>&lt;Zone&gt;</code> y <code>&lt;Total-Worker-Nodes&gt;</code> con los valores que anotó en el paso anterior:</p>
<pre><code class="language-bash">cd ~/training-data-analyst/CPB100/lab3b
bash authorize_dataproc.sh &lt;Cluster-Name&gt; &lt;Zone&gt; &lt;Total-Worker-Nodes&gt;
</code></pre>
<p>Cuando se le solicite, escriba <strong>Y</strong> y presione <strong>Enter</strong> para continuar.</p>
</li>
</ol>
<h2>Tarea 5: Ejecute el modelo del AA</h2>
<p>Para crear un modelo entrenado y aplicarlo a todos los usuarios del sistema, siga estos pasos:</p>
<ol>
<li>
<p>Edite el archivo de entrenamiento de modelos con <code>nano</code>:</p>
<pre><code class="language-bash">nano sparkml/train_and_apply.py
</code></pre>
</li>
<li>
<p>Cambie los campos marcados como #CHANGE en la parte superior del archivo (desplácese con la tecla de flecha hacia abajo) para que coincidan con su configuración de Cloud SQL (consulte las partes anteriores de este lab, en las que tomó nota de esa información). A fin de guardar el archivo, presione <strong>Ctrl+O</strong> y luego <strong>Enter</strong>. Por último, presione <strong>Ctrl+X</strong> para salir del archivo.</p>
</li>
<li>
<p>Use el siguiente comando para copiar este archivo a su depósito de Cloud Storage:</p>
<pre><code class="language-bash">gsutil cp sparkml/tr*.py gs://&lt;bucket-name&gt;/
</code></pre>
</li>
<li>
<p>En la consola de <strong>Dataproc</strong>, haga clic en <strong>Tareas</strong>.</p>
<p><img src="img/8508ce301ff584c3.png" alt="8508ce301ff584c3.png"></p>
</li>
<li>
<p>Haga clic en <strong>Enviar Tarea</strong>.</p>
</li>
<li>
<p>En <strong>Tipo de tarea</strong>, seleccione <strong>PySpark</strong> y, en <strong>Clase principal o .jar</strong>, especifique la ubicación del archivo de Python que subió a su depósito.</p>
<p><img src="img/e15bafe2c29956b5.png" alt="e15bafe2c29956b5.png"></p>
<p><code>gs://&lt;bucket-name&gt;/train_and_apply.py</code></p>
</li>
<li>
<p>Haga clic en <strong>Enviar</strong> y espere a que el estado del trabajo cambie de <code>En ejecución</code> a <code>Correcta</code> (esto puede demorar hasta 5 minutos).</p>
<p><img src="img/af55e2a91617b1d5.png" alt="af55e2a91617b1d5.png"></p>
<p>Si el trabajo muestra el estado <code>Failed</code>, consulte los registros para identificar y solucionar el problema. Es posible que deba volver a subir el archivo de Python modificado a Cloud Storage y clonar el trabajo con errores para volver a enviarlo.</p>
</li>
</ol>
<h2>Tarea 6: Explore las filas insertadas</h2>
<ol>
<li>
<p>En GCP Console, en el <strong>menú de navegación</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), haga clic en <strong>SQL</strong> en la sección Storage.</p>
</li>
<li>
<p>Haga clic en <strong>Rentals</strong> para ver los detalles de su instancia de Cloud SQL.</p>
</li>
<li>
<p>En la sección <strong>Conectarse a esta instancia</strong>, haga clic en <strong>Conectarse a través de Cloud Shell</strong>. Esto abrirá una nueva pestaña de Cloud Shell. Cuando esté en ella, presione <strong>Enter</strong>.</p>
<p>Su IP tardará unos minutos en incluirse en la lista blanca para las conexiones entrantes.</p>
</li>
<li>
<p>Cuando se le solicite, ingrese la contraseña raíz que configuró y, luego, presione <strong>Enter</strong>.</p>
</li>
<li>
<p>En el cuadro de MySQL, escriba lo siguiente:</p>
<pre><code class="language-bash">use recommendation_spark;
</code></pre>
<p>Con esta acción, se configura la base de datos en la sesión de MySQL.</p>
</li>
<li>
<p>Encuentre las recomendaciones de algunos usuarios:</p>
<pre><code class="language-bash">select r.userid, r.accoid, r.prediction, a.title, a.location, a.price, a.rooms, a.rating, a.type from Recommendation as r, Accommodation as a where r.accoid = a.id and r.userid = 10;
</code></pre>
<p>Estas son las cinco adaptaciones que le recomendaríamos. Tenga en cuenta que la calidad de las recomendaciones no es muy buena porque usamos un conjunto de datos muy reducido (observe que las calificaciones previstas no son muy altas). Aun así, en este lab se ilustra el proceso que debería seguir para crear recomendaciones de productos.</p>
</li>
</ol>
<fragment path="/fragments/endqwiklab"></fragment>
<h5>Última actualización del manual: 25 de febrero de 2019</h5>
<h5>Prueba más reciente del lab: 25 de febrero de 2019</h5>
<fragment path="/fragments/copyright"></fragment>
