<h1>Machine learning de recomendações com o Dataproc</h1>
<h2>Visão geral</h2>
<p>Neste laboratório, você usará o machine learning de recomendações com o Dataproc.</p>
<h3>Conteúdo do laboratório</h3>
<p>Neste laboratório, você aprenderá a:</p>
<ul>
<li>
<p>iniciar o Dataproc;</p>
</li>
<li>
<p>executar tarefas do SparkML com o Dataproc.</p>
</li>
</ul>
<h2>Introdução</h2>
<p>Neste laboratório, você usará o Dataproc para treinar o modelo de machine learning de recomendações com base nas classificações anteriores dos usuários. Em seguida, você aplicará esse modelo para criar uma lista de recomendações para cada usuário no banco de dados.</p>
<p>Neste laboratório, você aprenderá a:</p>
<ul>
<li>iniciar o Dataproc;</li>
<li>treinar e aplicar o modelo de ML criado no PySpark para criar recomendações de produtos;</li>
<li>explorar as linhas inseridas no Cloud SQL.</li>
</ul>
<fragment path="/fragments/startqwiklab"></fragment>
<h2>Configuração</h2>
<fragment path="/fragments/cloudshell"></fragment>
<h2>Tarefa 1: crie os recursos</h2>
<p>Para começar, vamos clonar o repositório de código, criar o intervalo de armazenamento no projeto do GCP e preparar alguns arquivos. Essas etapas são semelhantes às que você seguiu no laboratório anterior.</p>
<ol>
<li>
<p>No Cloud Shell, clone o repositório usando o seguinte comando:</p>
<pre><code class="language-bash">git clone https://github.com/GoogleCloudPlatform/training-data-analyst
</code></pre>
<p>Essa operação faz o download do código do GitHub.</p>
</li>
<li>
<p>Navegue até a pasta correspondente deste laboratório:</p>
<pre><code class="language-bash">cd training-data-analyst/CPB100/lab3a
</code></pre>
</li>
<li>
<p>No Console do GCP, no <strong>menu de navegação</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), clique em <strong>Storage</strong>.</p>
</li>
<li>
<p>Clique em <strong>Criar intervalo</strong>.</p>
</li>
<li>
<p>Em <strong>Name</strong>, digite o <strong>código do projeto</strong>. Selecione <strong>Definir permissões no nível do objeto e no nível do intervalo</strong> sob <strong>Modelo de controle de acesso</strong>, depois clique em <strong>Criar</strong>. Para encontrar o <strong>código do projeto</strong>, clique no projeto no menu superior do Console do GCP e copie o valor em <strong>ID</strong> do projeto selecionado.</p>
</li>
<li>
<p>Por fim, insira a definição da tabela e os arquivos de dados gradualmente no Cloud Storage, para que depois você possa importá-los para o Cloud SQL a partir do Cloud Shell no diretório lab3a. Para isso, basta digitar o seguinte, substituindo <code>&lt;BUCKET-NAME&gt;</code> pelo nome do intervalo criado:</p>
<pre><code class="language-bash">gsutil cp cloudsql/* gs://&lt;BUCKET-NAME&gt;/sql/
</code></pre>
</li>
<li>
<p>No Console do GCP, vá para "Storage", navegue até o intervalo e veja se os arquivos .sql e .csv estão no Cloud Storage.</p>
</li>
</ol>
<h2>Tarefa 2: crie uma instância do Cloud SQL</h2>
<p>Para criar uma instância do Cloud SQL, faça o seguinte:</p>
<ol>
<li>
<p>No Console do GCP, no <strong>menu de navegação</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), clique em <strong>SQL</strong> (na seção "Storage").</p>
</li>
<li>
<p>Clique em <strong>Criar instância</strong>.</p>
</li>
<li>
<p><strong>Selecionar MySQL</strong>. Clique em <strong>Próxima</strong> se necessário. Clique em <strong>Configurar MySQL de desenvolvimento</strong> ou <strong>Escolher a Segunda geração</strong>.</p>
</li>
<li>
<p>Em <strong>Código da instância</strong>, digite <strong>rentals</strong>.</p>
<p><img src="img/ab1cdf08212ecadf.png" alt="ab1cdf08212ecadf.png"></p>
</li>
<li>
<p>Role para baixo e especifique a senha raiz.  Antes que você esqueça, anote a senha raiz. Não faça isso fora deste exercício.</p>
</li>
<li>
<p>Role para baixo e, Clique em <strong>Definir conectividade</strong> ou <strong>Mostrar opções de configuração</strong> primeiro (se necessário) em seguida Clique em <strong>Redes autorizadas</strong> > <strong>+ Adicionar rede</strong>.</p>
<p><img src="img/ad40ddbc1cfc0a81.png" alt="ad40ddbc1cfc0a81.png"></p>
</li>
<li>
<p>No Cloud Shell, verifique se você está no diretório <strong>lab3a</strong> e digite o seguinte para encontrar seu endereço IP:</p>
<pre><code class="language-bash">bash ./find_my_ip.sh
</code></pre>
</li>
<li>
<p>Na caixa de diálogo <strong>Nove rede</strong>, digite um <strong>nome</strong> opcional e insira a saída de <strong>rede</strong> da etapa anterior. Clique em <strong>Concluído</strong>.</p>
<p><img src="img/54d82efbc516bceb.png" alt="54d82efbc516bceb.png"></p>
<p><strong>Observação</strong>: se você perder sua VM do Cloud Shell devido à inatividade, será necessário autorizá-la novamente com o Cloud SQL. Para sua conveniência, o lab3a inclui um script chamado <strong>authorize_cloudshell.sh</strong> que pode ser executado.</p>
</li>
<li>
<p>Clique em <strong>Criar</strong> para criar a instância. O provisionamento da instância do Cloud SQL levará cerca de um minuto.</p>
</li>
<li>
<p>Anote o <strong>Endereço IP público</strong> da sua instância do Cloud SQL, mostrado na janela do navegador.</p>
</li>
</ol>
<h2>Tarefa 3: crie e preencha as tabelas</h2>
<p>Para importar as definições de tabela do Cloud Storage, siga estas etapas:</p>
<ol>
<li>
<p>Clique em <strong>rentals</strong> para ver os detalhes sobre a instância do Cloud SQL.</p>
</li>
<li>
<p>Clique em <strong>Importar</strong>.</p>
</li>
<li>
<p>Clique em <strong>Parcurar</strong>. Uma lista de intervalos será exibida. Clique no intervalo criado, navegue para <strong>sql</strong>, clique em <strong>table_creation.sql</strong> e em <strong>Selecionar</strong>.</p>
</li>
<li>
<p>Clique em <strong>Importar</strong>.</p>
</li>
<aside class="warning"><p>
<strong>Nota</strong>: Se importação de arquivos <code>.sql</code> or <code>.csv</code> falha na primeira tentativa, tente novamente.
</p>
</aside>
<li>
<p>Em seguida, para importar arquivos CSV do Cloud Storage, clique em <strong>Importar</strong>.</p>
</li>
<li>
<p>Clique em <strong>Parcurar</strong>, navegue até <strong>sql</strong>, clique em <strong>accommodation.csv</strong> e clique em <strong>Selecionar</strong>.</p>
</li>
<li>
<p>Preencha o restante da caixa de diálogo com estas informações:</p>
<p>Em <strong>Banco de dados</strong>, selecione <strong>recommendation_spark</strong>.</p>
<p>Em <strong>Tabela</strong>, digite <strong>Accommodation</strong>.</p>
<p><img src="img/c899adb7fdb39f17.png" alt="c899adb7fdb39f17.png"></p>
</li>
<li>
<p>Clique em <strong>Importar</strong>.</p>
</li>
<li>
<p>Repita a importação (etapas 5 a 8) para <strong>rating.csv</strong>, mas digite <strong>Rating</strong> para <strong>Tabela</strong>.</p>
</li>
</ol>
<h2>Tarefa 4: inicie o Dataproc</h2>
<p>Para iniciar o Dataproc e configurá-lo para que cada máquina do cluster possa acessar o Cloud SQL, faça o seguinte:</p>
<ol>
<li>
<p>No Console do GCP, no <strong>menu de navegação</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), clique em <strong>SQL</strong> e observe a região da sua instância do Cloud SQL:</p>
<p><img src="img/fc8f254ae64a75b4.png" alt="fc8f254ae64a75b4.png"></p>
<p>Na captura de tela acima, a região é <code>us-central1</code>.</p>
</li>
<li>
<p>No Console do GCP, no <strong>menu de navegação</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), clique em <strong>Dataproc</strong> e em <strong>Enable API</strong>, se solicitado. Depois de ativar, clique em <strong>Criar cluster</strong>.</p>
</li>
<li>
<p>Deixe o Region como é, por exemplo global, Modifique a zona para que ela esteja na mesma região da sua instância do Cloud SQL. Isso minimizará a latência de rede entre o cluster e o banco de dados.</p>
</li>
<li>
<p>Para <strong>Nó principal</strong>, em <strong>Tipo de máquina</strong>, selecione <strong>2 vCPU (n1-standard-2)</strong>.</p>
</li>
<li>
<p>Para <strong>Nós de trabalho</strong>, em <strong>Tipo de máquina</strong>, selecione <strong>2 vCPU (n1-standard-2)</strong>.</p>
</li>
<li>
<p>Deixe todos os outros valores como o padrão e clique em <strong>Criar</strong>.  O provisionamento do seu cluster levará um ou dois minutos.</p>
</li>
<li>
<p>Anote os dados de <strong>Nome</strong>, <strong>Zona</strong> e <strong>Total de nós de trabalho</strong> do seu cluster.</p>
</li>
<li>
<p>No Cloud Shell, navegue até a pasta que corresponde a este laboratório e autorize todos os nós do Dataproc a acessar sua instância do Cloud SQL, substituindo <code>&lt;Cluster-Name&gt;</code>, <code>&lt;Zone&gt;</code> e <code>&lt;Total-Worker-Nodes&gt;</code> pelos valores observados na etapa anterior:</p>
<pre><code class="language-bash">cd ~/training-data-analyst/CPB100/lab3b
bash authorize_dataproc.sh &lt;Cluster-Name&gt; &lt;Zone&gt; &lt;Total-Worker-Nodes&gt;
</code></pre>
<p>Quando solicitado, pressione <strong>Y</strong> e <strong>Enter</strong> para continuar.</p>
</li>
</ol>
<h2>Tarefa 5: execute o modelo de ML</h2>
<p>Para criar um modelo treinado e aplicá-lo a todos os usuários do sistema, faça o seguinte:</p>
<ol>
<li>
<p>Edite o arquivo de treinamento do modelo com o comando <code>nano</code>:</p>
<pre><code class="language-bash">nano sparkml/train_and_apply.py
</code></pre>
</li>
<li>
<p>Altere os campos marcados como "#CHANGE" na parte superior do arquivo (role para baixo com a tecla de seta para baixo) para corresponder à configuração do seu Cloud SQL. Consulte as seções anteriores deste laboratório, em que você anotou essas informações. Em seguida, pressione <strong>Ctrl+O</strong> e <strong>Enter</strong> para salvar o arquivo e pressione <strong>Ctrl+X</strong> para fechar o arquivo.</p>
</li>
<li>
<p>Copie o arquivo para seu intervalo do Cloud Storage com o seguinte comando:</p>
<pre><code class="language-bash">gsutil cp sparkml/tr*.py gs://&lt;bucket-name&gt;/
</code></pre>
</li>
<li>
<p>No console do <strong>Dataproc</strong>, clique em <strong>Tarefas</strong>.</p>
<p><img src="img/8508ce301ff584c3.png" alt="8508ce301ff584c3.png"></p>
</li>
<li>
<p>Clique em <strong>Enviar Job</strong>.</p>
</li>
<li>
<p>Em <strong>Tipo de tarefa</strong>, selecione <strong>PySpark</strong> e, em <strong>Arquivo python principal</strong>, especifique o local do arquivo Python que você enviou para o intervalo.</p>
<p><img src="img/e15bafe2c29956b5.png" alt="e15bafe2c29956b5.png"></p>
<p><code>gs://&lt;bucket-name&gt;/train_and_apply.py</code></p>
</li>
<li>
<p>Clique em <strong>Enviar</strong> e aguarde até que o status da tarefa mude de <code>Em execução</code> para <code>Finalizado</code>. Isso levará até cinco minutos.</p>
<p><img src="img/af55e2a91617b1d5.png" alt="af55e2a91617b1d5.png"></p>
<p>Caso a tarefa fique com status <code>Failed</code>, solucione o problema com os registros e corrija os erros. Talvez seja necessário fazer novamente o upload do arquivo Python alterado para o Cloud Storage e clonar a tarefa com falha para reenviá-lo.</p>
</li>
</ol>
<h2>Tarefa 6: explore as linhas inseridas</h2>
<ol>
<li>
<p>No Console do GCP, no <strong>menu de navegação</strong> (<img alt="mainmenu.png" src="img/mainmenu.png">), clique em <strong>SQL</strong> (na seção "Storage").</p>
</li>
<li>
<p>Clique em <strong>rentals</strong> para ver detalhes sobre a instância do Cloud SQL.</p>
</li>
<li>
<p>Na seção <strong>Conectar-se a esta instância</strong>, clique em <strong>Conectar usando o Cloud Shel</strong>. Isso iniciará a nova guia Cloudshell. Na guia Cloudshell, pressione <strong>Enter</strong>.</p>
<p>Levará alguns minutos para incluir seu IP na lista de permissões para a conexão de entrada.</p>
</li>
<li>
<p>Quando solicitado, digite a senha raiz configurada e pressione <strong>Enter</strong>.</p>
</li>
<li>
<p>No prompt do mysql, digite:</p>
<pre><code class="language-bash">use recommendation_spark;
</code></pre>
<p>Isso define o banco de dados na sessão do mysql.</p>
</li>
<li>
<p>Veja as recomendações para um usuário:</p>
<pre><code class="language-bash">select r.userid, r.accoid, r.prediction, a.title, a.location, a.price, a.rooms, a.rating, a.type from Recommendation as r, Accommodation as a where r.accoid = a.id and r.userid = 10;
</code></pre>
<p>Essas são as cinco acomodações que nós recomendaríamos a ele. A qualidade das recomendações não é excelente porque nosso conjunto de dados é muito pequeno. Observe que as classificações previstas não são muito altas. Ainda assim, neste laboratório você pode ver o processo usado para criar recomendações de produto.</p>
</li>
</ol>
<fragment path="/fragments/endqwiklab"></fragment>
<h5>Manual atualizado em 25 de Fevereiro de 2019</h5>
<h5>Laboratório testado em 25 de Fevereiro de 2019</h5>
<fragment path="/fragments/copyright"></fragment>
